"""
Target Model of Adversarial Attacks generated by PGM.
    Reference:
        Alireza Bahramali, Milad Nasr, Amir Houmansadr, Dennis Goeckel, Don Towsley
        "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems"
        https://arxiv.org/abs/2102.00918
"""
__author__ = "plexix <byx1029@bupt.edu.cn>"

# External Includes
import torch.nn as nn

# Internal Includes
from .base import Model
from rfml.nn.layers import Flatten, PowerNormalization

class Target(Model):
    def __init__(self, input_samples: int, n_classes: int):
        super().__init__(input_samples, n_classes)

        self.preprocess = PowerNormalization()

        self.conv1 = nn.Conv2d(
                in_channels=1,
                out_channels=256,
                kernel_size=(1, 3),
                stride=(1, 1),
                padding=(0, 2),
        )
        self.a1 = nn.LeakyReLU()
        self.n1 = nn.BatchNorm2d(256)

        self.conv2 = nn.Conv2d(
                in_channels=256,
                out_channels=80,
                kernel_size=(2, 3),
                stride=(1, 1),
                padding=(0, 2),
        )
        self.a2 = nn.Softmax()
        self.n2 = nn.BatchNorm2d(80)

        self.flatten = Flatten(), # Flatten the input layer down to 1-d

        self.dense1 = nn.Linear(80 * 1 * input_samples, 10560)
        self.a3 = nn.LeakyReLU()
        self.n3 = nn.BatchNorm1d(10560)

        self.dense2 = nn.Linear(10560, 256)
        self.a4 = nn.Softmax()
        self.n4 = nn.BatchNorm1d(256)

        self.dense3 = nn.Linear(256, n_classes)


    def forward(self, x):
        x = self.preprocess(x)
        
        x = self.conv1(x)
        x = self.a1(x)
        x = self.n1(x) 

        x = self.conv2(x)
        x = self.a2(x)
        x = self.n2(x)

        x = self.flatten(x)

        x = self.dense1(x)
        x = self.a3(x)
        x = self.n3(x)

        x = self.dense2(x)
        x = self.a4(x)
        x = self.n4(x)

        x = self.dense3(x)

        return x

    def _freeze(self):
        """Freeze all of the parameters except for the dense layers.
        """
        for name, module in self.named_children():
            if "dense" not in name and "n3" not in name and "n4" not in name:
                for p in module.parameters():
                    p.requires_grad = False

    def _unfreeze(self):
        """Re-enable training of all parameters in the network.
        """
        for p in self.parameters():
            p.requires_grad = True